{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\trraa\\documents\\repos\\youtube-summarizer\\.venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.4/139.4 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "   ---------------------------------------- 0.0/214.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 214.4/214.4 kB 13.6 MB/s eta 0:00:00\n",
      "Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.3/2.3 MB 73.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 49.5 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Video Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "\n",
    "import requests\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "def list_most_recent_videos(days: int, channel: str):\n",
    "    today = datetime.datetime.today()\n",
    "    diff = datetime.timedelta(days=-days)\n",
    "    target = today + diff\n",
    "    target = target.strftime(\"%Y%m%d\")\n",
    "    params = {\n",
    "        \"dateafter\": target, \n",
    "        \"extractor-args\": {\"youtubetab\": {\"approximate_date\": ['']}},\n",
    "        \"progress\": True,\n",
    "        # \"write-pages\": True,\n",
    "    }\n",
    "\n",
    "    with YoutubeDL(params=params) as ydl:\n",
    "        info = ydl.extract_info(channel, download=False)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240615'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.datetime.today()\n",
    "diff = datetime.timedelta(days=-7)\n",
    "(today + diff).strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/@CivilNetTV/videos\n",
      "[youtube:tab] @CivilNetTV/videos: Downloading webpage\n",
      "[download] Downloading playlist: CIVILNET - Videos\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 1: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 2: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 3: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 4: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 5: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 6: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 7: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 8: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 9: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 10: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 11: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 12: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 13: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 14: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 15: Downloading API JSON\n",
      "[youtube:tab] UCxPJyX0oQWmqZ5akbuN5DTg page 16: Downloading API JSON\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mlist_most_recent_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.youtube.com/@CivilNetTV/videos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[100], line 21\u001b[0m, in \u001b[0;36mlist_most_recent_videos\u001b[1;34m(days, channel)\u001b[0m\n\u001b[0;32m     13\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdateafter\u001b[39m\u001b[38;5;124m\"\u001b[39m: target, \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextractor-args\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoutubetab\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproximate_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]}},\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprogress\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite-pages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m YoutubeDL(params\u001b[38;5;241m=\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m ydl:\n\u001b[1;32m---> 21\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mydl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1595\u001b[0m, in \u001b[0;36mYoutubeDL.extract_info\u001b[1;34m(self, url, download, ie_key, extra_info, process, force_generic_extractor)\u001b[0m\n\u001b[0;32m   1593\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ExistingVideoReached()\n\u001b[0;32m   1594\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__extract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_info_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1597\u001b[0m     extractors_restricted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowed_extractors\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1606\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1605\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1606\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1607\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (DownloadCancelled, LazyList\u001b[38;5;241m.\u001b[39mIndexError, PagedList\u001b[38;5;241m.\u001b[39mIndexError):\n\u001b[0;32m   1608\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1762\u001b[0m, in \u001b[0;36mYoutubeDL.__extract_info\u001b[1;34m(self, url, ie, download, extra_info, process)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process:\n\u001b[0;32m   1761\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_video(ie_result)\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_ie_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mie_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ie_result\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1891\u001b[0m, in \u001b[0;36mYoutubeDL.process_ie_result\u001b[1;34m(self, ie_result, download, extra_info)\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_thumbnails(ie_result)\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__process_playlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mie_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_playlist_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1966\u001b[0m, in \u001b[0;36mYoutubeDL.__process_playlist\u001b[1;34m(self, ie_result, download)\u001b[0m\n\u001b[0;32m   1964\u001b[0m     ie_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequested_entries\u001b[39m\u001b[38;5;124m'\u001b[39m], ie_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1966\u001b[0m     entries \u001b[38;5;241m=\u001b[39m resolved_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(entries)\n\u001b[0;32m   1967\u001b[0m     n_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(resolved_entries)\n\u001b[0;32m   1968\u001b[0m     ie_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequested_entries\u001b[39m\u001b[38;5;124m'\u001b[39m], ie_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresolved_entries)) \u001b[38;5;129;01mor\u001b[39;00m ([], [])\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\utils\\_utils.py:735\u001b[0m, in \u001b[0;36morderedSet.<locals>._iter\u001b[1;34m()\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter\u001b[39m():\n\u001b[0;32m    734\u001b[0m     seen \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Do not use set since the items can be unhashable\u001b[39;00m\n\u001b[1;32m--> 735\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\utils\\_utils.py:2418\u001b[0m, in \u001b[0;36mPlaylistEntries.get_requested_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mydl\u001b[38;5;241m.\u001b[39mreport_warning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIgnoring playliststart and playlistend because playlistitems was given\u001b[39m\u001b[38;5;124m'\u001b[39m, only_once\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_playlist_items(playlist_items):\n\u001b[1;32m-> 2418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\n\u001b[0;32m   2420\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\utils\\_utils.py:2480\u001b[0m, in \u001b[0;36mPlaylistEntries.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2480\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mIndexError:\n\u001b[0;32m   2482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_exhausted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\utils\\_utils.py:2453\u001b[0m, in \u001b[0;36mPlaylistEntries._getter.<locals>.get_entry\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_entry\u001b[39m(i):\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mydl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_extraction_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mydl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (LazyList\u001b[38;5;241m.\u001b[39mIndexError, PagedList\u001b[38;5;241m.\u001b[39mIndexError):\n\u001b[0;32m   2455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mIndexError()\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1606\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1605\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1606\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1607\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (DownloadCancelled, LazyList\u001b[38;5;241m.\u001b[39mIndexError, PagedList\u001b[38;5;241m.\u001b[39mIndexError):\n\u001b[0;32m   1608\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\utils\\_utils.py:2453\u001b[0m, in \u001b[0;36mPlaylistEntries._getter.<locals>.get_entry.<locals>.<lambda>\u001b[1;34m(_, i)\u001b[0m\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_entry\u001b[39m(i):\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mydl)\u001b[38;5;241m.\u001b[39m_handle_extraction_exceptions(\u001b[38;5;28;01mlambda\u001b[39;00m _, i: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mydl, i)\n\u001b[0;32m   2454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (LazyList\u001b[38;5;241m.\u001b[39mIndexError, PagedList\u001b[38;5;241m.\u001b[39mIndexError):\n\u001b[0;32m   2455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mIndexError()\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\utils\\_utils.py:2218\u001b[0m, in \u001b[0;36mLazyList.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m   2216\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m, stop \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mextend(itertools\u001b[38;5;241m.\u001b[39mislice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterable, n))\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[idx]\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\extractor\\youtube.py:5039\u001b[0m, in \u001b[0;36mYoutubeTabBaseInfoExtractor._entries\u001b[1;34m(self, tab, item_id, ytcfg, account_syncid, visitor_data)\u001b[0m\n\u001b[0;32m   5036\u001b[0m seen_continuations\u001b[38;5;241m.\u001b[39madd(continuation_token)\n\u001b[0;32m   5037\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_api_headers(\n\u001b[0;32m   5038\u001b[0m     ytcfg\u001b[38;5;241m=\u001b[39mytcfg, account_syncid\u001b[38;5;241m=\u001b[39maccount_syncid, visitor_data\u001b[38;5;241m=\u001b[39mvisitor_data)\n\u001b[1;32m-> 5039\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitem_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mitem_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m page \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpage_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mytcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_get_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontinuationContents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monResponseReceivedActions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monResponseReceivedEndpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n\u001b[0;32m   5045\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\extractor\\youtube.py:968\u001b[0m, in \u001b[0;36mYoutubeBaseInfoExtractor._extract_response\u001b[1;34m(self, item_id, query, note, headers, ytcfg, check_get_keys, ep, fatal, api_hostname, default_client)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfatal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mytcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mytcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_hostname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ExtractorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    975\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mcause, network_exceptions):\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\extractor\\youtube.py:616\u001b[0m, in \u001b[0;36mYoutubeBaseInfoExtractor._call_api\u001b[1;34m(self, ep, query, video_id, fatal, headers, note, errnote, context, api_key, api_hostname, default_client)\u001b[0m\n\u001b[0;32m    613\u001b[0m     real_headers\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[0;32m    614\u001b[0m api_key \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configuration_arg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minnertube_key\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m], ie_key\u001b[38;5;241m=\u001b[39mYoutubeIE\u001b[38;5;241m.\u001b[39mie_key(), casesense\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    615\u001b[0m            \u001b[38;5;129;01mor\u001b[39;00m api_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_api_key(default_client\u001b[38;5;241m=\u001b[39mdefault_client))\n\u001b[1;32m--> 616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_api_hostname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_hostname\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mdefault_client\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/youtubei/v1/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mep\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfatal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfatal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrnote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrnote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreal_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprettyPrint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\extractor\\common.py:1133\u001b[0m, in \u001b[0;36mInfoExtractor.__create_download_methods.<locals>.download_content\u001b[1;34m(self, url_or_request, video_id, note, errnote, transform_source, fatal, encoding, data, headers, query, expected_status, impersonate, require_impersonation)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_source\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# The method is fetched by name so subclasses can override _download_..._handle\u001b[39;00m\n\u001b[1;32m-> 1133\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\extractor\\common.py:1093\u001b[0m, in \u001b[0;36mInfoExtractor.__create_download_methods.<locals>.download_handle\u001b[1;34m(self, url_or_request, video_id, note, errnote, transform_source, fatal, encoding, data, headers, query, expected_status, impersonate, require_impersonation)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_handle\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_request, video_id, note\u001b[38;5;241m=\u001b[39mnote, errnote\u001b[38;5;241m=\u001b[39merrnote, transform_source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1091\u001b[0m                     fatal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, query\u001b[38;5;241m=\u001b[39m{}, expected_status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1092\u001b[0m                     impersonate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, require_impersonation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 1093\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_webpage_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrnote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrnote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfatal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfatal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimpersonate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpersonate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire_impersonation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequire_impersonation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\extractor\\common.py:954\u001b[0m, in \u001b[0;36mInfoExtractor._download_webpage_handle\u001b[1;34m(self, url_or_request, video_id, note, errnote, fatal, encoding, data, headers, query, expected_status, impersonate, require_impersonation)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(url_or_request, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    952\u001b[0m     url_or_request \u001b[38;5;241m=\u001b[39m url_or_request\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 954\u001b[0m urlh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_webpage\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrnote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfatal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mimpersonate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpersonate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire_impersonation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequire_impersonation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m urlh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fatal\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\extractor\\common.py:890\u001b[0m, in \u001b[0;36mInfoExtractor._request_webpage\u001b[1;34m(self, url_or_request, video_id, note, errnote, fatal, data, headers, query, expected_status, impersonate, require_impersonation)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_warning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; if you encounter errors, then \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, only_once\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_downloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m network_exceptions \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, HTTPError):\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:4142\u001b[0m, in \u001b[0;36mYoutubeDL.urlopen\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   4139\u001b[0m clean_headers(req\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_director\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSupportingHandlers \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   4144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ue \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39munsupported_errors:\n\u001b[0;32m   4145\u001b[0m         \u001b[38;5;66;03m# FIXME: This depends on the order of errors.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\networking\\common.py:117\u001b[0m, in \u001b[0;36mRequestDirector.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSending request via \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhandler\u001b[38;5;241m.\u001b[39mRH_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RequestError:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\networking\\_helper.py:208\u001b[0m, in \u001b[0;36mwrap_request_errors.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsupportedRequest \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mhandler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\networking\\common.py:337\u001b[0m, in \u001b[0;36mRequestHandler.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request, Request):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected an instance of Request\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\yt_dlp\\networking\\_requests.py:329\u001b[0m, in \u001b[0;36mRequestsRH._send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    326\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_instance(cookiejar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cookiejar(request))\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 329\u001b[0m     requests_res \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTooManyRedirects \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    341\u001b[0m     max_redirects_exceeded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = list_most_recent_videos(2, 'https://www.youtube.com/@CivilNetTV/videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/@adumb_codes/videos\n",
      "[youtube:tab] @adumb_codes/videos: Downloading webpage\n",
      "[download] Downloading playlist: adumb - Videos\n",
      "[youtube:tab] Playlist adumb - Videos: Downloading 6 items of 6\n",
      "[download] Downloading item 1 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=JheGL6uSF-4\n",
      "[youtube] JheGL6uSF-4: Downloading webpage\n",
      "[youtube] JheGL6uSF-4: Downloading ios player API JSON\n",
      "[youtube] JheGL6uSF-4: Downloading m3u8 information\n",
      "[download] Downloading item 2 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vLOgzi5y9xE\n",
      "[youtube] vLOgzi5y9xE: Downloading webpage\n",
      "[youtube] vLOgzi5y9xE: Downloading ios player API JSON\n",
      "[youtube] vLOgzi5y9xE: Downloading m3u8 information\n",
      "[download] Downloading item 3 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=mTPqIuEM-ug\n",
      "[youtube] mTPqIuEM-ug: Downloading webpage\n",
      "[youtube] mTPqIuEM-ug: Downloading ios player API JSON\n",
      "[youtube] mTPqIuEM-ug: Downloading m3u8 information\n",
      "[download] Downloading item 4 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=EOaPb9wrgDY\n",
      "[youtube] EOaPb9wrgDY: Downloading webpage\n",
      "[youtube] EOaPb9wrgDY: Downloading ios player API JSON\n",
      "[youtube] EOaPb9wrgDY: Downloading m3u8 information\n",
      "[download] Downloading item 5 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=mM_dC1HVAQ4\n",
      "[youtube] mM_dC1HVAQ4: Downloading webpage\n",
      "[youtube] mM_dC1HVAQ4: Downloading ios player API JSON\n",
      "[youtube] mM_dC1HVAQ4: Downloading m3u8 information\n",
      "[download] Downloading item 6 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=fAHMBiI6rIk\n",
      "[youtube] fAHMBiI6rIk: Downloading webpage\n",
      "[youtube] fAHMBiI6rIk: Downloading ios player API JSON\n",
      "[youtube] fAHMBiI6rIk: Downloading m3u8 information\n",
      "[download] Finished downloading playlist: adumb - Videos\n"
     ]
    }
   ],
   "source": [
    "info = list_most_recent_videos(0, \"https://www.youtube.com/@adumb_codes/videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.json', 'w') as file:\n",
    "    file.write(json.dumps(YoutubeDL().sanitize_info(info), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(info['entries'])):\n",
    "    print(\"Title:\", info['entries'][i]['title'])\n",
    "    print(\"Description:\", info['entries'][i]['description'])\n",
    "    print(\"Link to captions (en-orig):\", info['entries'][i]['automatic_captions']['en-orig'][0]['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(info['entries'][0]['automatic_captions']['en-orig'][0]['url'])\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(captions: dict) -> str:\n",
    "    text = []\n",
    "    for event in captions['events']:\n",
    "        if 'segs' in event:\n",
    "            for segment in event['segs']:\n",
    "                text.append(segment['utf8'])\n",
    "    return ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_text(response.json()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, pipeline\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "bart = BartForConditionalGeneration.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    "    # device_map='cuda',\n",
    "    ignore_mismatched_sizes=True,\n",
    ").to('cuda')\n",
    "# bart = pipeline('summarization', model=model_name, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trraa\\Documents\\repos\\youtube-summarizer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text = get_text(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text_batches: list) -> str:\n",
    "    if isinstance(text_batches, str):\n",
    "        text_batches = [text_batches]\n",
    "    inputs = tokenizer(text_batches, max_length=1024, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to('cuda')\n",
    "    summary_ids = bart.generate(inputs['input_ids'], num_beams=2, min_length=0, max_length=100)\n",
    "    result = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The graph is the culmination of months of work thousands of lines of code and an absurd amount of computation time to create a visual representation of Wikipedia. Each circle represents one of the 6.3 million English Wikipedia articles and these are the nearly 200 million links between these articles that form the network of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Wikipedia. The idea behind finding these communities is to group similar articles together since in the theory of knowledge theory articles which are more closelylinked to each other should also have more similar content'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get total approximate token length of the whole text\n",
    "whole_inputs = tokenizer([text], return_tensors='pt', truncation=False, add_special_tokens=False)\n",
    "num_tokens = len(whole_inputs['input_ids'][0])\n",
    "print(num_tokens)\n",
    "math.ceil(num_tokens / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17877\n",
      "4469\n",
      "17876\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(text) // 4)\n",
    "print(len(text) // 4 * 4)  # Close..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 12196,    47,  ..., 50118, 34464,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" it looks like this 46,000\\nincoming links is a lot but it's still a\\nrelatively small amount when compared to\\nthe most reference articles on Wikipedia\\nfor example over 100,000 articles linked\\nto the article for World War I however\\neven that is significantly less than the\\n189,000 articles that link to the\\narticle for World War II on a side note\\nI find it interesting how the graphs for\\nWorld War I and World War II are similar\\nto each other showing how articles with\\na similar type of content also share\\nsimilar links despite the article for\\nWorld War II being referenced nearly\\n200,000 times it's only the fourth most\\nlink to article on the English Wikipedia\\nthe second most referenced article has\\nover 240,000 links to it and it belongs\\nto the article for association football\\nor soccer the interesting thing about\\nthis graph is that even though it has\\nmore links most of the Links come from\\nwithin the same Community for football\\narticles with a lot of links originating\\nfrom articles on football players and\\nteams there's still one more article\\nleft which is referenced more than any\\nother article on Wikipedia linked to by\\nnearly 280,000 other articles the\\narticle with the single biggest impact\\non the Wikipedia graph is the United\\nStates that's right the most referenced\\narticle on Wikipedia isn't about a\\nfamous person or a historical event it's\\nabout a country in fact I found that 38%\\nof all articles on Wikipedia make\\nreference to articles of countries and\\nthis made curious what if I made a map\\nwhere the size of each dot is\\nproportional to the number of links to\\nits corresponding Wikipedia article it\\nmay be hard to determine a pattern of\\nwhich countries have more links to them\\nbut I think it becomes a lot clearer\\nwhen I highlight these countries\\naccording to Wikipedia these are the top\\n25 countries from which people\\ncontribute to the English Wikipedia\\nyou'll notice a fairly consistent\\noverlap with the highlighted countries\\nhaving the largest circles there are\\nsome exceptions like Iraq\\nwhich has many thousands of articles for\\nvillages and counties in Iran that were\\ncreated by a bot but for the most part\\nit's pretty consistent people will\\nnaturally want to write about things\\nthat interest them and things they're\\nfamiliar with and many times that will\\nend up linking back to the country they\\nlive in so along with the United States\\nbeing a global superpower and one of the\\nmost populated countries it also has the\\nmost contributors to the English\\nWikipedia so it makes sense that it's\\nlinked to more than any other\\narticle\\n[Music]\\nso now that we understand the colors of\\nthe graph and the size of the circles I\\nwant to talk about the inspiration for\\nthis whole\\nproject if you haven't heard of it the\\nWikipedia race or Wikipedia game is a\\ngame where you try to get from one\\nWikipedia page to another other by only\\nclicking links within the Articles often\\ntimes you are racing against other\\npeople or Trying to minimize the number\\nof clicks it takes for example if you\\nwanted to get from the article for\\nPokémon to the article for ancient Egypt\\nyou could do so in two clicks first by\\nclicking on the link to pets in the\\nPokémon article and then the link for\\nancient Egypt in the pet\\narticle often times this game is played\\nby ignoring links in the references and\\nsee also sections of Articles so when I\\nconstruct this the graph I also ignored\\nlinks in these sections since these are\\nnot necessarily a part of the\\narticle one question I've always\\nwondered when playing this game is does\\na path of links exist from every article\\nto every other article on Wikipedia well\\nit turns out the simple answer to that\\nquestion is no and the reason is\\norphans on Wikipedia an orphan is an\\narticle which has no other articles that\\nlink to it if one of these orphaned\\narticles was selected as the target\\narticle from for a Wikipedia race then\\nlet's just say you'd be playing for a\\nvery long time in total I found over\\n350,000 articles or about 5% of all\\nWikipedia articles which were orphaned\\nanother reason why a path doesn't exist\\nfrom every article to every other\\narticle is because of dead\\nends dead- end articles are articles\\nwhich have no links to any other\\narticles on Wikipedia if you started at\\na dead-end page or somehow found\\nyourself at a dead-end page during a\\nWikipedia race then you'd be stuck with\\nno way to get to any other article these\\nare quite a bit rarer than orphaned\\narticles with only about 6,000 articles\\nbeing dead ends now wait if orphaned\\narticles exist and Dead End articles\\nexist Does that imply the existence of\\ndeadend orphaned articles well it turns\\nout that a little over\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(whole_inputs['input_ids'][0][1024:2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"what you are looking at is a visual\\nrepresentation of Wikipedia a website\\nthat I can almost guarantee you've used\\nat some point in your life each circle\\nrepresents one of the 6.3 million\\nEnglish Wikipedia articles and these are\\nthe nearly 200 million links between\\nthese articles that form the network of\\nWikipedia this graph is the culmination\\nof months of work thousands of lines of\\ncode and an absurd amount of computation\\ntime and I know what you might be\\nthinking what a complete waste of time\\nbecause other than looking mildly\\ninteresting we can't really extract any\\nuseful information from this graph in\\nits current state but I promise that if\\nyou stay with me and watch this video\\nnot only will you come to understand\\nthis graph but you just might learn a\\ncouple of interesting things along the\\n[Music]\\nway\\nlet's start by understanding the\\ndifferent colors of this graph each\\ncolor represents different communities\\nwhich were algorithmically determined a\\ncommunity is a group of nodes or in this\\ncase articles which are more tightly\\nlinked to each other than they are to\\nother articles in the rest of the\\nnetwork in total the algorithm detected\\n44 different communities the idea behind\\nfinding these communities is to group\\nsimilar articles together since in\\ntheory articles which are more closely\\nlinked to each other should also have\\nmore similar cont content in order to\\ntest this Theory I looked at the\\ncategories for each of the articles in a\\ngiven community and found the most\\ncommon categories within that Community\\nfor example if we look at Community\\nnumber three which has over 760,000\\narticles we'll find that the most common\\ncategories of articles are related to\\npolitics and law it's in this community\\nwhich we'd find the articles of the\\nUnited States presidents now let's look\\nat Community number five and we'll find\\nthat this community is all about music\\nand it's word you're most likely to find\\nall your favorite musicians Community\\nnumber 10 is where you'll find video\\ngames and there's a high likelihood that\\nevery video game you've ever played has\\na corresponding article in this\\ncommunity now having communities about\\npolitics music and video games makes\\nsense because they're both popular and\\nBroad subjects but not all of the\\ncommunities are so straightforward for\\nexample the top categories of community\\nnumber 11 are about space objects and\\nCommunity number 19 is related to or\\nregion\\npoliticians one of the most interesting\\nthings about the way the articles are\\nseparated into different communities is\\nthe way they can reflect Society for\\nexample Community number six is all\\nabout English and American movies and\\ntelevision but there are also two\\nseparate communities for Indian and\\nKorean movies and television this shows\\nthe popularity of both Indian and Korean\\nCinema and their distinct separation\\nfrom Western Cinema I thought it would\\nbe interesting to look at films like\\nparasite and rrr with which come from\\nKorea and India respectively but found\\nmassive success in America as well these\\nmovies were more closely linked to\\nAmerican Cinema than most other foreign\\nfilms moving on one of my favorite\\ncommunities in this graph is community\\nnumber 14 the top categories here are\\npretty evenly split between Canadian\\npeople and hockey which I guess just\\nshows how closely linked hockey and\\nCanada are to each other in these cases\\nit's easy to understand the way articles\\nhave been grouped into their different\\ncommunities but that's not always the\\ncase let's look at the general\\nclassifications for each Community with\\na notable amount of Articles what I find\\ninteresting about this list is that the\\ntop categories from half of these\\ncommunities are related to different\\nsports if you were to ask a human to\\nbroadly categorize all the articles on\\nWikipedia there's a good chance they\\nwould put most if not all the sports\\narticles into a single group while the\\nWikipedia Network would suggest that\\nthey are all quite separate from each\\nother\\n[Music]\\nso now we understand the colors of the\\ngraph and the layout of the graph but\\nwhat about the different sizes of each\\ncircle the size of each circle or node\\nis proportional to the amount of\\nincoming links to its corresponding\\narticle in other words the more times an\\narticle is linked to by other articles\\nthe bigger its node will be for example\\nthe article for basketball is referenced\\nby 44,000 other articles so it's going\\nto be bigger than the articles for free\\nagent and golf which are linked to less\\ntimes something interesting we can do\\nwith the graph is look at all of the\\nlinks to a specific article to see how\\nmuch of an impact it has on the overall\\ngraph for example the article for\\ncovid-19 is one of the fastest growing\\nand most link to articles with over\\n46,000 articles referencing it when\\nvisualized\",\n",
       " \" it looks like this 46,000\\nincoming links is a lot but it's still a\\nrelatively small amount when compared to\\nthe most reference articles on Wikipedia\\nfor example over 100,000 articles linked\\nto the article for World War I however\\neven that is significantly less than the\\n189,000 articles that link to the\\narticle for World War II on a side note\\nI find it interesting how the graphs for\\nWorld War I and World War II are similar\\nto each other showing how articles with\\na similar type of content also share\\nsimilar links despite the article for\\nWorld War II being referenced nearly\\n200,000 times it's only the fourth most\\nlink to article on the English Wikipedia\\nthe second most referenced article has\\nover 240,000 links to it and it belongs\\nto the article for association football\\nor soccer the interesting thing about\\nthis graph is that even though it has\\nmore links most of the Links come from\\nwithin the same Community for football\\narticles with a lot of links originating\\nfrom articles on football players and\\nteams there's still one more article\\nleft which is referenced more than any\\nother article on Wikipedia linked to by\\nnearly 280,000 other articles the\\narticle with the single biggest impact\\non the Wikipedia graph is the United\\nStates that's right the most referenced\\narticle on Wikipedia isn't about a\\nfamous person or a historical event it's\\nabout a country in fact I found that 38%\\nof all articles on Wikipedia make\\nreference to articles of countries and\\nthis made curious what if I made a map\\nwhere the size of each dot is\\nproportional to the number of links to\\nits corresponding Wikipedia article it\\nmay be hard to determine a pattern of\\nwhich countries have more links to them\\nbut I think it becomes a lot clearer\\nwhen I highlight these countries\\naccording to Wikipedia these are the top\\n25 countries from which people\\ncontribute to the English Wikipedia\\nyou'll notice a fairly consistent\\noverlap with the highlighted countries\\nhaving the largest circles there are\\nsome exceptions like Iraq\\nwhich has many thousands of articles for\\nvillages and counties in Iran that were\\ncreated by a bot but for the most part\\nit's pretty consistent people will\\nnaturally want to write about things\\nthat interest them and things they're\\nfamiliar with and many times that will\\nend up linking back to the country they\\nlive in so along with the United States\\nbeing a global superpower and one of the\\nmost populated countries it also has the\\nmost contributors to the English\\nWikipedia so it makes sense that it's\\nlinked to more than any other\\narticle\\n[Music]\\nso now that we understand the colors of\\nthe graph and the size of the circles I\\nwant to talk about the inspiration for\\nthis whole\\nproject if you haven't heard of it the\\nWikipedia race or Wikipedia game is a\\ngame where you try to get from one\\nWikipedia page to another other by only\\nclicking links within the Articles often\\ntimes you are racing against other\\npeople or Trying to minimize the number\\nof clicks it takes for example if you\\nwanted to get from the article for\\nPokémon to the article for ancient Egypt\\nyou could do so in two clicks first by\\nclicking on the link to pets in the\\nPokémon article and then the link for\\nancient Egypt in the pet\\narticle often times this game is played\\nby ignoring links in the references and\\nsee also sections of Articles so when I\\nconstruct this the graph I also ignored\\nlinks in these sections since these are\\nnot necessarily a part of the\\narticle one question I've always\\nwondered when playing this game is does\\na path of links exist from every article\\nto every other article on Wikipedia well\\nit turns out the simple answer to that\\nquestion is no and the reason is\\norphans on Wikipedia an orphan is an\\narticle which has no other articles that\\nlink to it if one of these orphaned\\narticles was selected as the target\\narticle from for a Wikipedia race then\\nlet's just say you'd be playing for a\\nvery long time in total I found over\\n350,000 articles or about 5% of all\\nWikipedia articles which were orphaned\\nanother reason why a path doesn't exist\\nfrom every article to every other\\narticle is because of dead\\nends dead- end articles are articles\\nwhich have no links to any other\\narticles on Wikipedia if you started at\\na dead-end page or somehow found\\nyourself at a dead-end page during a\\nWikipedia race then you'd be stuck with\\nno way to get to any other article these\\nare quite a bit rarer than orphaned\\narticles with only about 6,000 articles\\nbeing dead ends now wait if orphaned\\narticles exist and Dead End articles\\nexist Does that imply the existence of\\ndeadend orphaned articles well it turns\\nout that a little over\",\n",
       " \" 2,000 dead-end\\norphans exist on Wikipedia articles\\nwhich have no incoming or outgoing links\\non top of being the most depressed\\nsounding types of articles on Wikipedia\\nI can't even show you these articles on\\nthe graph because they completely mess\\nup the graphing algorithm causing the\\nrest of the graph to become too\\ncompressed and leading to these articles\\nbecoming lost and\\n[Music]\\nforgotten so now we know we can't create\\na path between every pair of articles on\\nWikipedia but these dead end and\\norphaned articles make up only a small\\npercentage of total Wikipedia Pages the\\nvast majority of the Wikipedia graph is\\nactually pretty well connected for\\nexample if you wanted to get from the\\nhairy ball theorem to Pepsi fruit juice\\nflood you could do that in just four\\nclicks if you wanted to you could go\\nfrom baby Jesus theft to buffalo buffalo\\nbuffalo buffalo buffalo buffalo buffalo\\nbuffalo in five clicks anyway the point\\nI'm trying to make is that for most\\npairs of articles on Wikipedia a path\\ndoes exist and you can usually get from\\none article to a completely unrelated\\narticle in a surprisingly small amount\\nof clicks and this made me think of the\\nSix Degrees of Separation concept the\\nidea that every person is just six or\\nless Social Links away from every other\\nperson I wondered if you ignored dead\\nend and orphan Pages could the same idea\\nbe applied to the articles of Wikipedia\\nwhile in order to test this I started by\\nselecting a random Wikipedia page in\\nthis case it was the article for Pluto\\nfor the sake of this visualization I'll\\nmake all the other dots the same size\\nand color now we can plot the articles\\nin the first degree of separation these\\nare articles that are directly linked to\\nby the Pluto article in total there are\\n255 next let's highlight the articles in\\nthe second degree of separation these\\nare the articles that are directly\\nlinked to by the articles in the first\\ndegree of separation\\nin total there are over 20,000 and you\\ncan already see how fast the number of\\nArticles is growing this growth really\\nexplodes in the third degree of\\nseparation in Just Three Degrees of\\nSeparation we've gone from a single\\narticle to reaching over 618,000\\narticles and this growth continues with\\nnearly 3 million articles being reached\\nin the fourth degree of\\nseparation at this point we've already\\nmanaged to reach over 3.6 million\\narticles in total which is over half of\\nall Wikipedia articles this causes\\nsomething interesting to happen in the\\nfifth degree of separation where the\\nnumber of Articles reached starts to\\ndecrease in hindsight it makes sense\\nwe've already reached the majority of\\narticles in the graph so the number of\\nArticles reached in the following degree\\nwill start to decrease and it continues\\nto decrease with the sixth degree of\\nseparation as well we've now reached\\nover 5.7 million articles which is a lot\\nbut it's still only about 90% of all\\narticles this means that while we can\\nreach the large majority of articles in\\n6° of Separation we can't reach all of\\nthem we can keep increasing the degrees\\nof separation until the growth becomes\\nnegligible we can view each degree of\\nseparation separately to really get an\\nidea of how many articles were in each\\ndegree of\\nseparation if we view the growth on a\\ngraph we can see it increases slowly at\\nfirst then very rapidly and then very\\nslowly again\\nof course this is just what happened\\nwhen I selected Pluto as the starting\\narticle so I tested this again with a\\nnumber of different articles articles\\nwhich had thousands of links in the\\nfirst degree and articles which only had\\na single link and they all followed the\\nsame pattern what's interesting is that\\nall the graphs start to flatten out\\naround the 7th or 8th degree of\\nseparation at the same number of\\nArticles at 5.85 million articles\\nreached this accounts for about 92% of\\nall articles the remaining 8% of\\narticles are unreachable from the rest\\nof the graph as we already discussed\\nabout 5.5% of these articles are orphans\\nthe remaining 2.5% are orphan groups\\nthese are groups of Articles which have\\nlinks between each other but are not\\nlinked to by any other articles many of\\nthese are groups of the articles for\\nvillages and towns in Iran but my\\nfavorite orphan group has to be of the\\nActon family consisting of four English\\nmembers of parliament during the 1300s\\nthese articles all make reference to\\neach other but are not linked to by any\\nother articles on\\nWikipedia oh and it just so happens that\\nthese articles and these four articles\\nalone make up the entirety of community\\nnumber\\n[Music]\\n\",\n",
       " \"42 so now we know that for in the large\\nmajority of cases a path exists between\\ntwo articles and it will almost always\\nbe eight or fewer links long but what is\\nthe average path length between two\\narticles to test this I randomly picked\\n10,000 pairs of Articles and calculated\\nthe path length for each of them on\\naverage the path length between two\\narticles was\\n4.8 it's worth noting that about 8% of\\nthe time a path did not exist this is\\nconsistent with how we found about 8% of\\nArticles to be unreachable from the main\\ngraph you'll also notice that paths with\\nlengths less than three and greater than\\neight were extremely rare in fact only\\none path of the 10,000 tested had a\\nlength of 10 this made me wonder what's\\nthe longest path between two articles on\\nWikipedia now as I already mentioned\\nfinding two articles whose shortest path\\nbetween them is 10 or more is extremely\\nrare only happening about\\n0.01% of the time therefore a path with\\na length of 15 would be incredibly rare\\na path with a length of 30 would seem\\npretty much impossible but what if I\\ntold you the longest path I found was\\nover 60 links\\nlong sorry did I say 60 I meant to say\\n160 166 to be exact this path starts at\\nthe article for athletics in the 1953\\nArab games and finishes at a list of\\nHighways number 999\\nthe reason this path is so long is\\nbecause the only way to reach the list\\nof Highways number\\n999 is to start at the list of Highways\\nnumbered\\n825 and then tediously click each\\nsuccessive number until you reach\\n999 it takes a really long time but it's\\nthe only way to connect these two\\narticles I guess in some ways it's kind\\nof like an actual highway I can't say\\nfor certain that this is the longest\\npath on Wikipedia as calculating every\\nsingle path is not not feasible but it's\\ncertainly one of the\\n[Music]\\nlongest those were pretty much the most\\ninteresting things I found in the\\nWikipedia graph but I wanted to talk\\nabout one last thing one last article\\n[Music]\\nactually at first glance Fanta cake\\nlooks like a normal lb short article but\\nthere's actually something pretty\\nspecial about it you see it only has one\\nlink Fanta cukin but when you click it\\nit actually redirects back to itself\\nredirect Pages exist on Wikipedia to\\nhelp people find Pages easier for\\nexample if I search for USA it\\nautomatically redirects me to the United\\nStates article the page for the USA is\\njust a redirect page in the case of\\nfukin it simply redirects to Fanta cake\\nbut for some reason the only link on\\nFanta cake is the Fanta cukin creating a\\nsort of self\\nLoop technically speaking Fanta cake is\\nactually a dead end because as I\\ndiscussed earlier there are no paths to\\nany other\\narticles I like to call this a disguised\\ndead end because it appears to have a\\nlink at first but upon closer inspection\\nit's really just linking back to itself\\nbut that's not what makes Fant cake\\nunique there's actually a handful of\\nDisguise dead ends you see what makes\\nFanta cake special is that it's also an\\norphan page making it a disguised\\ndead-end orphan the only one of its\\nkind at least it was when I started\\nmaking this video but since then it's\\nactually been edited and has links to\\nother Pages it's for the same reason\\nthat when you watch this video a lot of\\nthe information might be slightly\\nincorrect or outdated but I don't think\\nthat's necessarily a bad thing in fact\\nthat's the beauty of Wikipedia it's an\\nEver growing and Ever Changing network\\nof information a place where anyone has\\nthe power to free an article from an\\nexistence of\\nsolitude thank you to my sponsors on\\nGitHub who support the channel and allow\\nme to make videos like this one which\\ntook a lot of time and effort by\\nsponsoring in me on GitHub you get\\naccess to the code from all my videos\\nincluding this one Link in the\\ndescription if you're\\ninterested if you made it this far and\\nenjoyed the video then consider\\nsubscribing and leaving a like because\\nit really does help thanks for\\nwatching\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = math.ceil(num_tokens / 1024)\n",
    "token_batch_size = 1024\n",
    "char_batch_size = len(text) // num_batches\n",
    "text_batches = []\n",
    "for i in range(num_batches):\n",
    "    batch = tokenizer.decode(whole_inputs['input_ids'][0][i * token_batch_size : (i + 1) * token_batch_size])\n",
    "    text_batches.append(batch)\n",
    "# text_batches = [text[i * char_batch_size : (i + 1) * char_batch_size] for i in range(4)]\n",
    "text_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph is the culmination of months of work thousands of lines of code and an absurd amount of computation time to create a visual representation of Wikipedia. Each circle represents one of the 6.3 million English Wikipedia articles and these are the nearly 200 million links between articles that form the network of                Wikipedia. The idea behindfinding these communities is to group similar articles together since in the theory of knowledge theory articles which are more closelylinked to each other should also have more similar content.\n",
      "The most referenced article on Wikipedia isn't about a famous person or a historical event it's about a country. 38% of all articles on Wikipedia makereference to articles of countries and countries. The United States is referenced more than any other article onWikipedia linked to by nearly 280,000 other articles.\n",
      "2,000 dead-end and orphaned articles exist on Wikipedia articles. These articles have no incoming or outgoing links. The number of articles on Wikipedia is growing very rapidly.\n",
      "The average path length between two articles on Wikipedia is 4.42 links. The longest path I found was 60 links long. Fanta cake only has one link but when you click it actually redirects back to itself.\n",
      "The graph is the culmination of months of work thousands of lines of code and an absurd amount of computation time to create a visual representation of Wikipedia. Each circle represents one of the 6.3 million English Wikipedia articles and these are the nearly 200 million links between articles that form the network of Wikipedia. The average path length between two articles on Wikipedia is 4.42 links.\n"
     ]
    }
   ],
   "source": [
    "batch_summaries = summarize(text_batches)\n",
    "for summary in batch_summaries:\n",
    "    print(summary)\n",
    "\n",
    "final_summary = ' '.join(summarize('\\n'.join(batch_summaries))[0].split())\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_long_text(text: str, token_batch_size: int = 1024, return_partial_summaries=False) -> str:\n",
    "    # Get total approximate token length of the whole text\n",
    "    whole_inputs = tokenizer([text], return_tensors='pt', truncation=False, add_special_tokens=False)\n",
    "    num_tokens = len(whole_inputs['input_ids'][0])\n",
    "    num_batches = math.ceil(num_tokens / token_batch_size)\n",
    "\n",
    "    # Batchify text\n",
    "    text_batches = []\n",
    "    for i in range(num_batches):\n",
    "        batch = tokenizer.decode(whole_inputs['input_ids'][0][i * token_batch_size : (i + 1) * token_batch_size])\n",
    "        text_batches.append(batch)\n",
    "\n",
    "    # Generate summaries\n",
    "    batch_summaries = summarize(text_batches)\n",
    "    final_summary = ' '.join(summarize('\\n'.join(batch_summaries))[0].split())\n",
    "\n",
    "    if return_partial_summaries:\n",
    "        return final_summary, batch_summaries\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Using AI to Create the Perfect Keyboard\n",
      "Summary: The QWERTY keyboard was invented nearly 150 years ago by Christopher Latham. The keyboard layout has remained unchanged ever since.\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "print(\"Title:\", info['entries'][index]['title'])\n",
    "response = requests.get(info['entries'][index]['automatic_captions']['en-orig'][0]['url'])\n",
    "assert response.ok, response.status_code\n",
    "text = get_text(response.json())\n",
    "result = summarize_long_text(text, return_partial_summaries=True)\n",
    "print(\"Summary:\", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The QWERTY keyboard was invented nearly 150 years ago by Christopher Latham scholes. The keyboard layout has remained unchanged ever since. I simulated a keyboard layout that is more efficient by minimizing the total distance the fingers have to travel to type.',\n",
       " 'The genetic algorithm is used to create the next generation of keyboards. It uses the most efficient keyboards from each generation to create a new layout. The keyboard should become increasingly efficient over time as it mimics natural selection.',\n",
       " 'The AI uses a genetic algorithm to group the keys on a keyboard. The most commonly used keys are placed closer to the starting positions of the two fingers. The semicolon key is now in a closer position to the home Keys. The rst lne keyboard layout might be the best for typing Standard English.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Video\n",
    "Putting it together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_video(url: str):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
